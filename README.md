# TorchServe
TorchServe Demo to demonstrate how the model serving process inside a MLOps environment can work using a little example deploying an PyTorch ML model.

The project create three different Docker containers and will connect the environment with an AWS S3 bucket as a model-store to save the MAR files generated by the creation of the PyTorch models to then be used by TorchServe to be deployed inside a container exposing an API to consume the ML models using HTTP request
Containers generated:
* Model API: API that will connect the environment with AWS to push and pull the ML models generated.
* Model Creation: Container that will generate the ML models and save the result into the S3 bucket sending a POST request through the Model API.
* TorchServe: Container that will expose the TorchServe API and generate an API endpoint to consume the generated model.

## Setup
The project works in an environment with docker installed. 
If you don't have docker installed you can execute the docker installer script on this repo 
``` bash
bash dockerInstaller.sh <your-username>
```
### ENV Variable
It use one single environment variable where we can put the S3 Bucket name
``` bash
export BUCKET_NAME="<your-bucket-name>"
```


### Access Keys
To be able to connect with AWS we need to add the Access Keys of our user that has access with the S3 bucket.
``` csv
User Name,Access key ID,Secret access key,Default region name
default,<access-key-ID>,<secret-access-key>,<bucket-region-name>

```
You can get the Access Keys from AWS IAM.

If you don't know how you can check the next link: [Create AWS Access Keys](doc:https://aws.amazon.com/es/premiumsupport/knowledge-center/create-access-key/)

# Docker
Once we have all set up we can start creating the Docker Images and Containers.
## API Model 

### Create Docker Image
```bash
docker build --tag=model_serving_api . -f API/Dockerfile
```
### Create container
``` bash
docker run -d --name model_api -v /home/ubuntu/TorchServe/model/mr:/modelTrained -p 5000:5000 model_serving_api
```
## Create Model 
### Create Docker Image
``` bash
docker build --build-arg model_name="foodnet_resnet18" \
    --build-arg bucket_name=$BUCKET_NAME \
    --tag=model_serving_train . \
    -f model/Dockerfile
```
### Create container
``` bash
docker run -d --name model_train -v /home/ubuntu/TorchServe/model/mr:/home/model-server/modelTrained model_serving_train
```
## Model Serving 
### Create Docker Image
``` bash
docker build --build-arg model_name="foodnet_resnet18" \
    --build-arg bucket_name=$BUCKET_NAME \
    --tag=model_serving_model .
```
### Create Container
``` bash
docker run -d --name torchserve -p 8080:8080 model_serving_model
```
### Validate health of torchserve API
``` bash
curl http://localhost:8080/ping
```
### Execute model
``` bash
curl -X POST http://localhost:8080/predictions/foodnet -T sample/sample.jpg
```